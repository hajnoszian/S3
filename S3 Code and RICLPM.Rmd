---
title: "S3 RICLPM"
author: "Ian Hajnosz"
date: "2023-09-29"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(haven)#for .sav loads
library(tidyverse)
library(ggplot2)
library(viridis)
library(gridExtra)
```



# Loading Data
```{r}
df0 <- read_sav("For Ian.sav")

df1  <- read_sav("SEDC 2022-23 Phase 2 data Ian.sav")
```

Checking for ID consistency
```{r}
ID0 <- df0 |> 
  group_by(ID) |> 
  count(ID) |> 
  select(ID)#88 IDs in baseline

ID1 <- df1 |> 
  group_by(ID) |> 
  count(ID) |> 
  select(ID)#92 IDs in survey?
```
Do we have mismatches?

```{r}
ID0$number <- row.names(ID0)
ID1$number <- row.names(ID1)
IDs <- merge(ID0, ID1, by = "number", all = TRUE) #use this object to chart out the weird IDs here 
```

Actually wait, there is no other info I can use to confirm (mis) alignment in either direction. I cannot confirm that person 120 in the baseline should be 201 or is simply missing in the later stages

So we will lose some from the baseline to the daily merge

#### Data checks and Recoding
Baseline Recoding
```{r}
table(df0$gend)
table(df0$trans)
table(df0$ethn)
table(df0$ethn_TEXT)
table(df0$sexorn)
table(df0$myeduc)
table(df0$employ)
table(df0$relstat)
table(df0$Parstudy)

df0 <- 
  mutate(df0,
       gend = recode(factor(gend),
                     "0" = "Man",
                     "1" = "Woman",
                     "2" = "NonbinaryGenderQueer",
                     "4" = "AgenderGenderless"),
       trans = recode(factor(trans),
                      "0" = "No",
                      "1" = "Yes",
                      "2" = "DontKnow"),
       ethn = recode(factor(ethn),
                     "0" = "WhiteCaucasianAnglo",
                     "1" = "BlackAfricanCaribbean",
                     "2" = "HispanicLatinoaChicanoa",
                     "3" = "EastAsian",
                     "4" = "SouthAsian",
                     "5" = "SoutheastAsian",
                     "9" = "MixedMultipleEthnicGroups",
                     "10" = "Additional"),
       sexorn = recode(factor(sexorn),
                       "0" = "HeterosexualStraight",
                       "1" = "Gay",
                       "2" = "Lesbian",
                       "3" = "Queer",
                       "4" = "BisexualPansexual",
                       "6" = "Asexual"),
       myeduc = recode(factor(myeduc),
                       "2" = "UpperSecondary",
                       "3" = "VocationalDegree",
                       "4" = "UndergraduateDegree",
                       "5" = "MastersDegree",
                       "6" = "DoctorateProgressionalDegree",
                       "7" = "Other"),
       student = recode(factor(student),
                        "0" = "No",
                        "1" = "Yes"
                        ),
       employ = recode(factor(employ),
                       "0" = "No",
                       "1" = "PartTime",
                       "2" = "FullTime"),
       relstat = recode(factor(relstat),
                        "2" = "DatingExclusive",
                        "3" = "CommonLaw",
                        "5" = "Engaged",
                        "6" = "Married"),
       Parstudy = recode(factor(Parstudy),
                         "1" = "No",
                         "2" = "Yes")
       )
df0$ethn[df0$ethn_TEXT == "Mixed White and east Asian (but happy to be grouped in mixed)"] <- "MixedMultipleEthnicGroups"
```
### Data Merge and Cleaning
```{r}
#For the merge of the baseline and daily, I only need a few vars from the baseline to show up. This will be a large, wide dataset eventually so may as well start making it cleaner here and take just want I need
df <- df0 |> 
  select(ID, gend, rellength)

df <- merge(df, df1, by.x = "ID", by.y = "ID") #adding the demographic covariates to the daily data. Adding 2 vars to df1, essentially 

df <- df |> 
  rename(
    PartnerKM = a_p2whatkm1,
    FriendKM = a_p2whatkm2,
    FamilyKM = a_p2whatkm3,
    ColleKM = a_p2whatkm4,
    AcquaKM = a_p2whatkm5,
    StranKM = a_p2whatkm6,
    AnimaKM = a_p2whatkm7,
    NatureKM = a_p2whatkm8,
    MusicKM = a_p2whatkm9,
    NonFicKM = a_p2whatkm10,
    FicKM = a_p2whatkm11,
    OtherKM = a_p2whatkm12
  )
```

```{r}
df |> 
  filter(OtherKM == 1)

df[df$a_p2whatkm12_TEXT == "My pet Nala",]
df$OtherKM[df$a_p2whatkm12_TEXT == "My pet Nala"] <- 0 #this person already has Animal listed, so can remove the Other here
```


Do we have 6 days for everyone? How many people do we even have?
```{r}
df |> 
  group_by(ID) |> 
  count(ID)

#at least we have 6 data points for each person!
```

# Descriptives

```{r}
#I need baseline data, but using just the IDs that end up in the final merged df
o1 <- df1 |> 
  select(ID) |> 
  unique()

dfDem <- merge(df0, o1) #this should have the 87 folks from the final df, but in the baseline format (i.e. don't have to worry about recount tallys)
#Generally shouldnt be a problem, but lets just avoid that here just in case

dfDem$ID == unique(df$ID) #they're the same IDs! Woo!
```
##### Scaffolding Plotting Code

Histogram (dummy code)
```{r}
nrow(df) # = 352
#Age
p1 <- ggplot(df, aes(x = Age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Age Distribution (n=352)",
       x = "Age (Binwidth = 1)",
       y = "Count")
#Race
p2 <- ggplot(df, aes(x = Race)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5)) +
  labs(title = "Ethnicity/Race Distribution (n=352)",
       y = "Count")

grid.arrange(p1,p2, nrow = 1)
```

Density (dummy code)
```{r}
p1 <- ggplot(df, aes(KM_P, group = Cond, fill = Cond)) +
  geom_density(alpha = 0.6) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(x = "Physiological Signs",
       y = "Density")

p2 <- ggplot(df, aes(KM_CSR, group = Cond, fill = Cond)) +
  geom_density(alpha = 0.6) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(x = "CSR Appraisal",
       y = "Density")

p3 <- ggplot(df, aes(KM_M, group = Cond, fill = Cond)) +
  geom_density(alpha = 0.6) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(x = "Prosocial Motivation",
       y = "Density")

p4 <- ggplot(df, aes(KM_L, group = Cond, fill = Cond)) +
  geom_density(alpha = 0.6) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(x = "Lexical Label",
       y = "Density")

grid.arrange(p1,p2,p3,p4, nrow = 2)
```

Violin (dummy code)
```{r}
p1 <- ggplot(df, aes(Cond, ANS_1, fill = Cond)) +
  geom_violin() +
  geom_boxplot(width = 0.1, color = "black", alpha = 0.4) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(title = "Time 1 Autonomy Satisfaction",
       x = "",
       y = "")

p2 <- ggplot(df, aes(Cond, ANF_1, fill = Cond)) +
  geom_violin() +
  geom_boxplot(width = 0.1, color = "black", alpha = 0.4) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(title = "Time 1 Autonomy Frustration",
       x = "",
       y = "")
p3 <- ggplot(df, aes(Cond, RNS_1, fill = Cond)) +
  geom_violin() +
  geom_boxplot(width = 0.1, color = "black", alpha = 0.4) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(title = "Time 1 Relatedness Satisfaction",
       x = "",
       y = "")

p4 <- ggplot(df, aes(Cond, RNF_1, fill = Cond)) +
  geom_violin() +
  geom_boxplot(width = 0.1, color = "black", alpha = 0.4) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(title = "Time 1 Relatedness Frustration",
       x = "",
       y = "")
p5 <- ggplot(df, aes(Cond, CNS_1, fill = Cond)) +
  geom_violin() +
  geom_boxplot(width = 0.1, color = "black", alpha = 0.4) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(title = "Time 1 Competence Satisfaction",
       x = "",
       y = "")
p6 <- ggplot(df, aes(Cond, CNF_1, fill = Cond)) +
  geom_violin() +
  geom_boxplot(width = 0.1, color = "black", alpha = 0.4) +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(title = "Time 1 Competence Frustration",
       x = "",
       y = "")

grid.arrange(p1,p2,p3,p4,p5,p6, nrow = 3)
ggsave("Time 1 By Condition.png")
```


Boxplot (dummy code)
```{r}
df_plot <- df %>% 
  select("Cond", "ANS_1", "ANS_2") %>% 
  pivot_longer(cols = c("ANS_1", "ANS_2"),
               names_to = "Time",
               values_to = "Score")

p1 <- ggplot(df_plot, aes(x = Time, y = Score, fill = Cond)) +
  geom_boxplot() +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(y = "Score")

df_plot <- df %>% 
  select("Cond", "ANF_1", "ANF_2") %>% 
  pivot_longer(cols = c("ANF_1", "ANF_2"),
               names_to = "Time",
               values_to = "Score")

p2 <- ggplot(df_plot, aes(x = Time, y = Score, fill = Cond)) +
  geom_boxplot() +
  scale_fill_viridis(discrete=TRUE, name = "Condition")+
  labs(y = "Score")

grid.arrange(p1, p2, nrow = 1)
```

#### Gender & Trans
```{r}
prop.table(table(dfDem$gend))*100

p1 <- ggplot(dfDem, aes(x = gend)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5)) +
  labs(title = "Ethnicity",
       x="",
       y = "Count")

prop.table(table(dfDem$trans))*100
p2 <- ggplot(dfDem, aes(x = trans)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5)) +
  labs(title = "Transgender",
       x = "",
       y = "Count")
grid.arrange(p1,p2, nrow = 2)
```

#### Age
```{r}
dfDem |> 
  summarise(
    AgeM = mean(age, na.rm = T),
    AgeSD = sd(age, na.rm = T)
    )

ggplot(dfDem, aes(x = age)) +
  geom_histogram(binwidth = 1) +
  labs(title = "Age Distribution",
       x = "Age (Binwidth = 1)",
       y = "Count")

```

#### Ethnicity
```{r}
prop.table(table(dfDem$ethn))*100

ggplot(dfDem, aes(x = ethn)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5)) +
  labs(title = "Ethnicity",
       x = "",
       y = "Count")
```

#### Sexual Orientation
```{r}
prop.table(table(dfDem$sexorn))*100

ggplot(dfDem, aes(x = sexorn)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5)) +
  labs(title = "Sexual Orientation",
       x = "",
       y = "Count")
```
#### Education
```{r}
prop.table(table(dfDem$myeduc))*100

ggplot(dfDem, aes(x = myeduc)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5)) +
  labs(title = "Education",
       x = "",
       y = "Count")
```
#### Student
```{r}
prop.table(table(dfDem$student))*100

ggplot(dfDem, aes(x = student)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5)) +
  labs(title = "Student Status",
       x = "",
       y = "Count")
```

#### Employed
```{r}
prop.table(table(dfDem$employ))*100

ggplot(dfDem, aes(x = employ)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5)) +
  labs(title = "Employed",
       x = "",
       y = "Count")
```

#### Income
This is rather difficult since we have different currencies being reflected here. Skipping for now.

#### Relationship Status
```{r}
prop.table(table(dfDem$relstat))*100

ggplot(dfDem, aes(x = relstat)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 30, vjust = 0.5)) +
  labs(title = "Relationship Status",
       x = "",
       y = "Count")
```


#### Relationship Length
```{r}
dfDem |> 
  summarise(
    RelLenM = mean(rellength, na.rm = T),
    RelLenSD = sd(rellength, na.rm = T)
    )

ggplot(dfDem, aes(x = rellength)) +
  geom_histogram(binwidth = 2) +
  labs(title = "Month Distribution",
       x = "Months",
       y = "Count")

ggplot(dfDem, aes(x = rellength)) +
  geom_boxplot() +
  labs(title = "Month Boxplot",
     x = "Months",
     y = "")

#Density plot kinda sucks here
#ggplot(dfDem, aes(rellength)) +
#  geom_density(alpha = 0.6) +
#  labs(x = "Months in Relationship",
#       y = "Sample Density")

```

#### Living Situation
Kinda tricky to plot. TBD for now.

#### Partner in Study
```{r}
prop.table(table(dfDem$Parstudy))*100

ggplot(dfDem, aes(x = Parstudy)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 0, vjust = 0.5)) +
  labs(title = "Partner in Study",
       x = "",
       y = "Count")
```
### Necessary?

##### Attachment (Baseline)

##### PRI (Baseline)

##### Relationship Satisfaction (Baseline)

##### Commitment (Baseline)

##### Trust (Baseline)

##### Intimacy/Closeness (Baseline)

#RQ1
Descriptive evaluation of the elicitors and frequency of kama muta

### How often? All bar+(KM>0, any scale)
```{r}
df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 0, "Potential KM", "None")
  ) |> 
  count(KMmark) #So 429 out of 522 (82%) days potentially had a KM event

o1 <- df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 0, 1, 0)
    ) |> 
  group_by(ID) |> 
  mutate(
    Day = row_number()
  ) |> 
  select(ID, KMmark, Day)

o1 <- o1 |>
  pivot_wider(
    names_from = Day,
    names_prefix = "Day_",
    values_from = KMmark
  )
o1$KMtot <- rowSums(o1[2:7], na.rm = T)
o1 |> 
  ungroup(ID) |> 
  summarise(mean = mean(KMtot)) #note, this should be the exact same ratio as the above 82%. I.e. 4.93 days/6 is also == 82%
```
### How often? Low bar+ (>6, 1/4 of scale)
The 6 KM items are 0-4, meaning maximum points is 24 
```{r}
df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 6, "Potential KM", "None")
  ) |> 
  count(KMmark) #So 244 out of 522 (46.7%%) days potentially had a KM event

o1 <- df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 6, 1, 0)
    ) |> 
  group_by(ID) |> 
  mutate(
    Day = row_number()
  ) |> 
  select(ID, KMmark, Day)

o1 <- o1 |>
  pivot_wider(
    names_from = Day,
    names_prefix = "Day_",
    values_from = KMmark
  )
o1$KMtot <- rowSums(o1[2:7], na.rm = T)
o1 |> 
  ungroup(ID) |> 
  summarise(mean = mean(KMtot)) #note, this should be the exact same ratio as the above 46.%. I.e. 2.80 days/6 is also == 46.7%
```

### How often? M ed+ bar (>12, 1/2 of scale)
The 6 KM items are 0-4, meaning maximum points is 24 
```{r}
df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 12, "Potential KM", "None")
  ) |> 
  count(KMmark) #So 78 out of 522 (14.9%) days potentially had a KM event

o1 <- df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 12, 1, 0)
    ) |> 
  group_by(ID) |> 
  mutate(
    Day = row_number()
  ) |> 
  select(ID, KMmark, Day)

o1 <- o1 |>
  pivot_wider(
    names_from = Day,
    names_prefix = "Day_",
    values_from = KMmark
  )
o1$KMtot <- rowSums(o1[2:7], na.rm = T)
o1 |> 
  ungroup(ID) |> 
  summarise(mean = mean(KMtot)) #note, this should be the exact same ratio as the above 14.9%. I.e. .89 days/6 is also == 14.9%
```

### How often? high bar+ (>18, 3/4 of scale)
The 6 KM items are 0-4, meaning maximum points is 24 
```{r}
df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 18, "Potential KM", "None")
  ) |> 
  count(KMmark) #So 8 out of 522 (1.5%) days potentially had a KM event

o1 <- df |> 
  mutate(
    KMmark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 18, 1, 0)
    ) |> 
  group_by(ID) |> 
  mutate(
    Day = row_number()
  ) |> 
  select(ID, KMmark, Day)

o1 <- o1 |>
  pivot_wider(
    names_from = Day,
    names_prefix = "Day_",
    values_from = KMmark
  )
o1$KMtot <- rowSums(o1[2:7], na.rm = T)
o1 |> 
  ungroup(ID) |> 
  summarise(mean = mean(KMtot)) #note, this should be the exact same ratio as the above 1.5%. I.e. .09 days/6 is also == 1.5%
```


### What evoked the KM?

```{r}
o1 <- df |> 
  select(ID, PartnerKM, FriendKM, FamilyKM, ColleKM, AcquaKM, StranKM, AnimaKM, NatureKM, MusicKM, NonFicKM, FicKM, OtherKM) |> 
  group_by(ID) |> 
  mutate(
    Day = row_number()
  ) |> 
  pivot_longer(cols = !c(ID, Day), names_to = "Evoker") |> 
  filter(value == 1) |> 
  group_by(ID, Day) #1318 observations of KM evokers. 522 observations total, means people were reporting on average ~2.5 evokers per day
#Remember, people could report more than one KM evoker per each day

table(o1$Evoker) #Out of all those evokers, who came up the most?
round(prop.table(table(o1$Evoker))*100, 1)

ggplot(o1, aes(x = Evoker)) +
  geom_bar()
```

```{r}
#Same as code above, but with added markers for KM on that day
o1 <- df |> 
  select(ID, PartnerKM, FriendKM, FamilyKM, ColleKM, AcquaKM, StranKM, AnimaKM, NatureKM, MusicKM, NonFicKM, FicKM, OtherKM,
         a_p2km1, a_p2km2, a_p2km3, a_p2km4, a_p2km5 , a_p2km6) |> 
  group_by(ID) |> 
  mutate(
    Day = row_number(),
    Mark = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 0, 1, 0),
    MarkQtr = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 6, 1, 0),
    MarkHalf = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 12, 1, 0),
    Mark34 = ifelse(a_p2km1 + a_p2km2 + a_p2km3 + a_p2km4 + a_p2km5 + a_p2km6 > 18, 1, 0),
  ) |> 
  select(ID, Day, PartnerKM, FriendKM, FamilyKM, ColleKM, AcquaKM, StranKM, AnimaKM, NatureKM, MusicKM, NonFicKM, FicKM, OtherKM,
         Mark, MarkQtr, MarkHalf, Mark34) |> 
  pivot_longer(cols = !c(ID, Day, Mark, MarkQtr, MarkHalf, Mark34), names_to = "Evoker") |> 
  filter(value == 1) |> 
  group_by(ID, Day) #1318 observations of KM evokers. 522 observations total, means people were reporting on average ~2.5 evokers per day
#Remember, people could report more than one KM evoker per each day
all(o1$Mark == o1$value)#This is TRUE, which is good. Each all all BARE MINIMUM (mark) KM should have an evoker (value).


#All+ bar
sum(table(o1$Evoker[o1$Mark == 1]))
table(o1$Evoker[o1$Mark == 1])
round(prop.table(table(o1$Evoker[o1$Mark == 1]))*100,1)

#Low+ bar
sum(table(o1$Evoker[o1$MarkQtr == 1]))
table(o1$Evoker[o1$MarkQtr == 1])
round(prop.table(table(o1$Evoker[o1$MarkQtr == 1]))*100,1)

#Med+ bar
sum(table(o1$Evoker[o1$MarkHalf == 1]))
table(o1$Evoker[o1$MarkHalf == 1])
round(prop.table(table(o1$Evoker[o1$MarkHalf == 1]))*100,1)

#High+ bar
sum(table(o1$Evoker[o1$Mark34 == 1]))
table(o1$Evoker[o1$Mark34 == 1])
round(prop.table(table(o1$Evoker[o1$Mark34 == 1]))*100,1)


p1 <- o1 |> 
  filter(Mark == 1) |> 
  ggplot(aes(x = Evoker)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) +
  labs(title = "Evokers, All KM Days",
       x = "",
       y = "Count (n = 1318)")

p2 <- o1 |> 
  filter(MarkQtr == 1) |> 
  ggplot(aes(x = Evoker)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) +
  labs(title = "Evokers, Low+ KM Days",
       x = "",
       y = "Count (n = 870)")

p3 <- o1 |> 
  filter(MarkHalf == 1) |> 
  ggplot(aes(x = Evoker)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) +
  labs(title = "Evokers, Med+ KM Days",
       x = "",
       y = "Count (n = 292)") 

p4 <- o1 |> 
  filter(Mark34 == 1) |> 
  ggplot(aes(x = Evoker)) +
  geom_bar() +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) +
  labs(title = "Evokers, High+ KM Days",
       x = "",
       y = "Count (n = 33)") 

grid.arrange(p1,p2, p3,p4, nrow = 2)


o1 |> 
  mutate(
    MarkTotal = "Mark",
    MarkTotal = ifelse(MarkQtr == 1, "Qtr", MarkTotal),
    MarkTotal = ifelse(MarkHalf == 1, "Half", MarkTotal),
    MarkTotal = ifelse(Mark34 == 1, "ThreeQtr", MarkTotal),
    ) |> 
  ggplot(aes(Evoker, fill = MarkTotal)) +
  geom_bar(position = "stack") +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) +
  labs(title = "Evokers",
       x = "",
       y = "Count")


o1 |> 
  mutate(
    MarkTotal = "Mark",
    MarkTotal = ifelse(MarkQtr == 1, "Qtr", MarkTotal),
    MarkTotal = ifelse(MarkHalf == 1, "Half", MarkTotal),
    MarkTotal = ifelse(Mark34 == 1, "ThreeQtr", MarkTotal),
    MarkTotal = factor(MarkTotal, levels = c("Mark", "Qtr", "Half", "ThreeQtr"))
    ) |> 
  ggplot(aes(MarkTotal, fill = Evoker)) +
  geom_bar(position = "fill") +
  theme(axis.text.x = element_text(angle = 60, vjust = 0.5)) +
  labs(title = "Evokers",
       x = "KM Days",
       y = "Percentage")
```
Two things I'm learning here.
A) Partners are by far the most common elicitor of KM feels. This is definitely on track for what we'd expect.
B) Km evokers are pretty social (Partner, next fam & friends)...then **music** all of a sudden pops. That's cool!

#### Site comparison
Will probably have to do some basic t-tests/barplots/violin plots here for some comparison between sites of KM and PPR


# RQ2-3: Modeling 
## Reworking to Wide format
Will need to rework the dataset to be wide format here:
```{r}

```


## Model Approach

The overall steps are:

Model Fit (attempt to do those covariate models)
Tests of constraints over time: A) Factor invariance (since we use factors) B) Effects time-invariance, 

#### Factorial/Measurement invariance test (Mulder & Hamaker, 2020)
1) Configural invariance--no constraints on factor loadings over time on w/in unit factors -- does the model fit at all?
2) Weak Invariance -- constrain the factor loadings to be invariant, compare to unconstrained model. If non-sig, we can conclude the constraints are tenable. If sig, we cannot conclude weak invariance
3) Strong invariance -- constrain the intercepts of the model to be invariant, estimate the latent means from the 2nd occassion onward. 

See RICLPM1.ext3 for this stepwise process (just multiple indicator, but not latent)
See RICLPM5 for the latent version

I need to do a bit of both RICLPM1 for iterations, but RICLPM5 for the latent
#### Effects time-invariance
1) Constrain lagged regression coefficients to be time invariant, compare to unconstrained model. If non-sig test (i.e. the models are about the same) we can conclude the constraints are tenable and the dynamics are time in-variant. If not tenable, we can conclude there is a some developmental change across the time span (Mulder & Hamaker, 2020) 
2) Constrain the means (the RIs) to be time invariant, compare to unconstrained model. If non-sig, it is evidence the construct is stable at the population level for the duration of the study. If sig, it implies there is on average a change in variable over time, perhaps a developmental trend. (Mulder & Hamaker, 2020).

See RICLPM5 in the tutorial for this model code.






#### Coding Approach
I'll use this variable notation:
x11, x21, x31, x41 = xWaveVar. So waves 1 through 4 for x variable 1
x12, x22, x32, x42 = So waves 1 through 4 for x variable 2 etc.

## Base Model

### Fit and Factor Invariance
#### Configural
```{r}
base.m1 <- '

  #####################
  # MEASUREMENT MODEL #
  #####################
  
  # Factor models for km at 6 waves (UNconstrained)
  Fkm1 =~ 1*km11 + 1*km12 + 1*km13 + 1*km14 + 1*km15 + 1*km16
  Fkm2 =~ 1*km21 + 1*km22 + 1*km23 + 1*km24 + 1*km25 + 1*km26
  Fkm3 =~ 1*km31 + 1*km32 + 1*km33 + 1*km34 + 1*km35 + 1*km36
  Fkm4 =~ 1*km41 + 1*km42 + 1*km43 + 1*km44 + 1*km45 + 1*km46
  Fkm5 =~ 1*km51 + 1*km52 + 1*km53 + 1*km54 + 1*km55 + 1*km56
  Fkm6 =~ 1*km61 + 1*km62 + 1*km63 + 1*km64 + 1*km65 + 1*km66
  
  # Factor models for ppr at 6 waves (UNconstrained)
  Fppr1 =~ 1*ppr11 + 1*ppr12 + 1*ppr13 + 1*ppr14
  Fppr2 =~ 1*ppr21 + 1*ppr22 + 1*ppr23 + 1*ppr24
  Fppr3 =~ 1*ppr31 + 1*ppr32 + 1*ppr33 + 1*ppr34
  Fppr4 =~ 1*ppr41 + 1*ppr42 + 1*ppr43 + 1*ppr44
  Fppr5 =~ 1*ppr51 + 1*ppr52 + 1*ppr53 + 1*ppr54
  Fppr6 =~ 1*ppr61 + 1*ppr62 + 1*ppr63 + 1*ppr64
  
  ################
  # BETWEEN PART #
  ################
  
  # Create between factors (RIs)
  RIkm =~ 1*Fkm1 + 1*Fkm2 + 1*Fkm3 + 1*Fkm4 + 1*Fkm5 + 1*Fkm6
  RIppr =~ 1*Fppr1 + 1*Fppr2 + 1*Fppr3 + 1*Fppr4 + 1*Fppr5 + 1*Fppr6
  
  # Set residual variances of all Fkm and Fppr vars to 0
  # I think this specifies all variance of the factors goes to either w/n or b/n
  Fkm1 ~~ 0*Fkm1
  Fkm2 ~~ 0*Fkm2
  Fkm3 ~~ 0*Fkm3
  Fkm4 ~~ 0*Fkm4
  Fkm5 ~~ 0*Fkm5
  Fkm6 ~~ 0*Fkm6
  Fppr1 ~~ 0*Fppr1
  Fppr2 ~~ 0*Fppr2
  Fppr3 ~~ 0*Fppr3
  Fppr4 ~~ 0*Fppr4
  Fppr5 ~~ 0*Fppr5
  Fppr6 ~~ 0*Fppr6

  ###############
  # WITHIN PART #
  ###############

  # Create within-components
  WFkm1 =~ 1*Fkm1
  WFkm2 =~ 1*Fkm2
  WFkm3 =~ 1*Fkm3
  WFkm4 =~ 1*Fkm4
  WFkm5 =~ 1*Fkm5
  WFkm6 =~ 1*Fkm6
  
  WFppr1 =~ 1*Fppr1
  WFppr2 =~ 1*Fppr2
  WFppr3 =~ 1*Fppr3
  WFppr4 =~ 1*Fppr4
  WFppr5 =~ 1*Fppr5
  WFppr6 =~ 1*Fppr6
  
  # Specify lagged effects between within-person centered latent variables
  WFkm2 + WFppr2 ~ WFkm1 + WFppr1
  WFkm3 + WFppr3 ~ WFkm2 + WFppr2
  WFkm4 + WFppr4 ~ WFkm3 + WFppr3
  WFkm5 + WFppr5 ~ WFkm4 + WFppr4
  WFkm6 + WFppr6 ~ WFkm5 + WFppr5
  
  # Estimate correlations within same wave
  WFkm1 ~~ WFppr1 #I am pretty sure they missed this line in the og tutorial
  WFkm2 ~~ WFppr2
  WFkm3 ~~ WFppr3
  WFkm4 ~~ WFppr4
  WFkm5 ~~ WFppr5
  WFkm6 ~~ WFppr6
  
  ##########################
  # ADDITIONAL CONSTRAINTS #
  ##########################
  
  # Set correlations between between-factors (RIs) and within-factors at wave 1 at 0
  RIkm + RIppr ~~ 0*WFkm1 + 0*WFppr1
  
'


base.m1.fit <- cfa(base.m1,
                   data = data,
                   missing = "ML"
                   )
summary(base.m1.fit)
```
do I need to specify covariance between the RIs? ANd do I need to specify RI covariance to themselves? (Or are those both handled by downstream covariances? I remember vaguely something about that in a paper somewhere)
EDIT:
By using cfa() (eg rather than sem or lavaan), lavaan default will be to include freely correlated residuals 

#### Configural W/Demographic Covariate
```{r}
base.m1D <- '

  #####################
  # MEASUREMENT MODEL #
  #####################
  
  # Factor models for km at 6 waves (UNconstrained)
  Fkm1 =~ 1*km11 + 1*km12 + 1*km13 + 1*km14 + 1*km15 + 1*km16
  Fkm2 =~ 1*km21 + 1*km22 + 1*km23 + 1*km24 + 1*km25 + 1*km26
  Fkm3 =~ 1*km31 + 1*km32 + 1*km33 + 1*km34 + 1*km35 + 1*km36
  Fkm4 =~ 1*km41 + 1*km42 + 1*km43 + 1*km44 + 1*km45 + 1*km46
  Fkm5 =~ 1*km51 + 1*km52 + 1*km53 + 1*km54 + 1*km55 + 1*km56
  Fkm6 =~ 1*km61 + 1*km62 + 1*km63 + 1*km64 + 1*km65 + 1*km66
  
  # Factor models for ppr at 6 waves (UNconstrained)
  Fppr1 =~ 1*ppr11 + 1*ppr12 + 1*ppr13 + 1*ppr14
  Fppr2 =~ 1*ppr21 + 1*ppr22 + 1*ppr23 + 1*ppr24
  Fppr3 =~ 1*ppr31 + 1*ppr32 + 1*ppr33 + 1*ppr34
  Fppr4 =~ 1*ppr41 + 1*ppr42 + 1*ppr43 + 1*ppr44
  Fppr5 =~ 1*ppr51 + 1*ppr52 + 1*ppr53 + 1*ppr54
  Fppr6 =~ 1*ppr61 + 1*ppr62 + 1*ppr63 + 1*ppr64
  
  ################
  # BETWEEN PART #
  ################
  
  # Create between factors (RIs)
  RIkm =~ 1*Fkm1 + 1*Fkm2 + 1*Fkm3 + 1*Fkm4 + 1*Fkm5 + 1*Fkm6 + GENDER + RELLENGTH
  RIppr =~ 1*Fppr1 + 1*Fppr2 + 1*Fppr3 + 1*Fppr4 + 1*Fppr5 + 1*Fppr6 + GENDER + RELLENGTH
  
  # Set residual variances of all Fkm and Fppr vars to 0
  # I think this specifies all variance of the factors goes to either w/n or b/n
  Fkm1 ~~ 0*Fkm1
  Fkm2 ~~ 0*Fkm2
  Fkm3 ~~ 0*Fkm3
  Fkm4 ~~ 0*Fkm4
  Fkm5 ~~ 0*Fkm5
  Fkm6 ~~ 0*Fkm6
  Fppr1 ~~ 0*Fppr1
  Fppr2 ~~ 0*Fppr2
  Fppr3 ~~ 0*Fppr3
  Fppr4 ~~ 0*Fppr4
  Fppr5 ~~ 0*Fppr5
  Fppr6 ~~ 0*Fppr6

  ###############
  # WITHIN PART #
  ###############

  # Create within-components
  WFkm1 =~ 1*Fkm1
  WFkm2 =~ 1*Fkm2
  WFkm3 =~ 1*Fkm3
  WFkm4 =~ 1*Fkm4
  WFkm5 =~ 1*Fkm5
  WFkm6 =~ 1*Fkm6
  
  WFppr1 =~ 1*Fppr1
  WFppr2 =~ 1*Fppr2
  WFppr3 =~ 1*Fppr3
  WFppr4 =~ 1*Fppr4
  WFppr5 =~ 1*Fppr5
  WFppr6 =~ 1*Fppr6
  
  # Specify lagged effects between within-person centered latent variables
  WFkm2 + WFppr2 ~ WFkm1 + WFppr1
  WFkm3 + WFppr3 ~ WFkm2 + WFppr2
  WFkm4 + WFppr4 ~ WFkm3 + WFppr3
  WFkm5 + WFppr5 ~ WFkm4 + WFppr4
  WFkm6 + WFppr6 ~ WFkm5 + WFppr5
  
  # Estimate correlations within same wave
  WFkm1 ~~ WFppr1 #I am pretty sure they missed this line in the og tutorial
  WFkm2 ~~ WFppr2
  WFkm3 ~~ WFppr3
  WFkm4 ~~ WFppr4
  WFkm5 ~~ WFppr5
  WFkm6 ~~ WFppr6
  
  ##########################
  # ADDITIONAL CONSTRAINTS #
  ##########################
  
  # Set correlations between between-factors (RIs) and within-factors at wave 1 at 0
  RIkm + RIppr ~~ 0*WFkm1 + 0*WFppr1
  
'


base.m1.fit <- cfa(base.m1,
                   data = data,
                   missing = "ML"
                   )
summary(base.m1.fit)
```


#### Weak
```{r}
base.m2 <- '

  #####################
  # MEASUREMENT MODEL #
  #####################
  
  # Factor models for km at 6 waves (Constrained)
  Fkm1 =~ a*km11 + b*km12 + c*km13 + d*km14 + e*km15 + f*km16
  Fkm2 =~ a*km21 + b*km22 + c*km23 + d*km24 + e*km25 + f*km26
  Fkm3 =~ a*km31 + b*km32 + c*km33 + d*km34 + e*km35 + f*km36
  Fkm4 =~ a*km41 + b*km42 + c*km43 + d*km44 + e*km45 + f*km46
  Fkm5 =~ a*km51 + b*km52 + c*km53 + d*km54 + e*km55 + f*km56
  Fkm6 =~ a*km61 + b*km62 + c*km63 + d*km64 + e*km65 + f*km66
  
  # Factor models for ppr at 6 waves (Constrained)
  Fppr1 =~ g*ppr11 + h*ppr12 + i*ppr13 + j*ppr14
  Fppr2 =~ g*ppr21 + h*ppr22 + i*ppr23 + j*ppr24
  Fppr3 =~ g*ppr31 + h*ppr32 + i*ppr33 + j*ppr34
  Fppr4 =~ g*ppr41 + h*ppr42 + i*ppr43 + j*ppr44
  Fppr5 =~ g*ppr51 + h*ppr52 + i*ppr53 + j*ppr54
  Fppr6 =~ g*ppr61 + h*ppr62 + i*ppr63 + j*ppr64
  
  ################
  # BETWEEN PART #
  ################
  
  # Create between factors (RIs)
  RIkm =~ 1*Fkm1 + 1*Fkm2 + 1*Fkm3 + 1*Fkm4 + 1*Fkm5 + 1*Fkm6
  RIppr =~ 1*Fppr1 + 1*Fppr2 + 1*Fppr3 + 1*Fppr4 + 1*Fppr5 + 1*Fppr6
  
  # Set residual variances of all Fkm and Fppr vars to 0
  # I think this specifies all variance of the factors goes to either w/n or b/n
  Fkm1 ~~ 0*Fkm1
  Fkm2 ~~ 0*Fkm2
  Fkm3 ~~ 0*Fkm3
  Fkm4 ~~ 0*Fkm4
  Fkm5 ~~ 0*Fkm5
  Fkm6 ~~ 0*Fkm6
  Fppr1 ~~ 0*Fppr1
  Fppr2 ~~ 0*Fppr2
  Fppr3 ~~ 0*Fppr3
  Fppr4 ~~ 0*Fppr4
  Fppr5 ~~ 0*Fppr5
  Fppr6 ~~ 0*Fppr6

  ###############
  # WITHIN PART #
  ###############

  # Create within-components
  WFkm1 =~ 1*Fkm1
  WFkm2 =~ 1*Fkm2
  WFkm3 =~ 1*Fkm3
  WFkm4 =~ 1*Fkm4
  WFkm5 =~ 1*Fkm5
  WFkm6 =~ 1*Fkm6
  
  WFppr1 =~ 1*Fppr1
  WFppr2 =~ 1*Fppr2
  WFppr3 =~ 1*Fppr3
  WFppr4 =~ 1*Fppr4
  WFppr5 =~ 1*Fppr5
  WFppr6 =~ 1*Fppr6
  
  # Specify lagged effects between within-person centered latent variables
  WFkm2 + WFppr2 ~ WFkm1 + WFppr1
  WFkm3 + WFppr3 ~ WFkm2 + WFppr2
  WFkm4 + WFppr4 ~ WFkm3 + WFppr3
  WFkm5 + WFppr5 ~ WFkm4 + WFppr4
  WFkm6 + WFppr6 ~ WFkm5 + WFppr5
  
  # Estimate correlations within same wave
  WFkm1 ~~ WFppr1 #I am pretty sure they missed this line in the og tutorial
  WFkm2 ~~ WFppr2
  WFkm3 ~~ WFppr3
  WFkm4 ~~ WFppr4
  WFkm5 ~~ WFppr5
  WFkm6 ~~ WFppr6
  
  ##########################
  # ADDITIONAL CONSTRAINTS #
  ##########################
  
  # Set correlations between between-factors (RIs) and within-factors at wave 1 at 0
  RIkm + RIppr ~~ 0*WFkm1 + 0*WFppr1
  
'


base.m2.fit <- cfa(base.m2,
                   data = data,
                   missing = "ML"
                   )
summary(base.m2.fit)
```

#### Weak w/Demographic Covariate

```{r}
base.m2D <- '

  #####################
  # MEASUREMENT MODEL #
  #####################
  
  # Factor models for km at 6 waves (Constrained)
  Fkm1 =~ a*km11 + b*km12 + c*km13 + d*km14 + e*km15 + f*km16
  Fkm2 =~ a*km21 + b*km22 + c*km23 + d*km24 + e*km25 + f*km26
  Fkm3 =~ a*km31 + b*km32 + c*km33 + d*km34 + e*km35 + f*km36
  Fkm4 =~ a*km41 + b*km42 + c*km43 + d*km44 + e*km45 + f*km46
  Fkm5 =~ a*km51 + b*km52 + c*km53 + d*km54 + e*km55 + f*km56
  Fkm6 =~ a*km61 + b*km62 + c*km63 + d*km64 + e*km65 + f*km66
  
  # Factor models for ppr at 6 waves (Constrained)
  Fppr1 =~ g*ppr11 + h*ppr12 + i*ppr13 + j*ppr14
  Fppr2 =~ g*ppr21 + h*ppr22 + i*ppr23 + j*ppr24
  Fppr3 =~ g*ppr31 + h*ppr32 + i*ppr33 + j*ppr34
  Fppr4 =~ g*ppr41 + h*ppr42 + i*ppr43 + j*ppr44
  Fppr5 =~ g*ppr51 + h*ppr52 + i*ppr53 + j*ppr54
  Fppr6 =~ g*ppr61 + h*ppr62 + i*ppr63 + j*ppr64
  
  ################
  # BETWEEN PART #
  ################
  
  # Create between factors (RIs)
  RIkm =~ 1*Fkm1 + 1*Fkm2 + 1*Fkm3 + 1*Fkm4 + 1*Fkm5 + 1*Fkm6 + GENDER + RELLENGTH
  RIppr =~ 1*Fppr1 + 1*Fppr2 + 1*Fppr3 + 1*Fppr4 + 1*Fppr5 + 1*Fppr6 + GENDER + RELLENGTH
  
  # Set residual variances of all Fkm and Fppr vars to 0
  # I think this specifies all variance of the factors goes to either w/n or b/n
  Fkm1 ~~ 0*Fkm1
  Fkm2 ~~ 0*Fkm2
  Fkm3 ~~ 0*Fkm3
  Fkm4 ~~ 0*Fkm4
  Fkm5 ~~ 0*Fkm5
  Fkm6 ~~ 0*Fkm6
  Fppr1 ~~ 0*Fppr1
  Fppr2 ~~ 0*Fppr2
  Fppr3 ~~ 0*Fppr3
  Fppr4 ~~ 0*Fppr4
  Fppr5 ~~ 0*Fppr5
  Fppr6 ~~ 0*Fppr6

  ###############
  # WITHIN PART #
  ###############

  # Create within-components
  WFkm1 =~ 1*Fkm1
  WFkm2 =~ 1*Fkm2
  WFkm3 =~ 1*Fkm3
  WFkm4 =~ 1*Fkm4
  WFkm5 =~ 1*Fkm5
  WFkm6 =~ 1*Fkm6
  
  WFppr1 =~ 1*Fppr1
  WFppr2 =~ 1*Fppr2
  WFppr3 =~ 1*Fppr3
  WFppr4 =~ 1*Fppr4
  WFppr5 =~ 1*Fppr5
  WFppr6 =~ 1*Fppr6
  
  # Specify lagged effects between within-person centered latent variables
  WFkm2 + WFppr2 ~ WFkm1 + WFppr1
  WFkm3 + WFppr3 ~ WFkm2 + WFppr2
  WFkm4 + WFppr4 ~ WFkm3 + WFppr3
  WFkm5 + WFppr5 ~ WFkm4 + WFppr4
  WFkm6 + WFppr6 ~ WFkm5 + WFppr5
  
  # Estimate correlations within same wave
  WFkm1 ~~ WFppr1 #I am pretty sure they missed this line in the og tutorial
  WFkm2 ~~ WFppr2
  WFkm3 ~~ WFppr3
  WFkm4 ~~ WFppr4
  WFkm5 ~~ WFppr5
  WFkm6 ~~ WFppr6
  
  ##########################
  # ADDITIONAL CONSTRAINTS #
  ##########################
  
  # Set correlations between between-factors (RIs) and within-factors at wave 1 at 0
  RIkm + RIppr ~~ 0*WFkm1 + 0*WFppr1
  
'


base.m2.fit <- cfa(base.m2,
                   data = data,
                   missing = "ML"
                   )
summary(base.m2.fit)
```
#### Strong

```{r}
base.m3 <- '

  #####################
  # MEASUREMENT MODEL #
  #####################
  
  # Factor models for km at 6 waves (Constrained)
  Fkm1 =~ a*km11 + b*km12 + c*km13 + d*km14 + e*km15 + f*km16
  Fkm2 =~ a*km21 + b*km22 + c*km23 + d*km24 + e*km25 + f*km26
  Fkm3 =~ a*km31 + b*km32 + c*km33 + d*km34 + e*km35 + f*km36
  Fkm4 =~ a*km41 + b*km42 + c*km43 + d*km44 + e*km45 + f*km46
  Fkm5 =~ a*km51 + b*km52 + c*km53 + d*km54 + e*km55 + f*km56
  Fkm6 =~ a*km61 + b*km62 + c*km63 + d*km64 + e*km65 + f*km66
  
  # Factor models for ppr at 6 waves (Constrained)
  Fppr1 =~ g*ppr11 + h*ppr12 + i*ppr13 + j*ppr14
  Fppr2 =~ g*ppr21 + h*ppr22 + i*ppr23 + j*ppr24
  Fppr3 =~ g*ppr31 + h*ppr32 + i*ppr33 + j*ppr34
  Fppr4 =~ g*ppr41 + h*ppr42 + i*ppr43 + j*ppr44
  Fppr5 =~ g*ppr51 + h*ppr52 + i*ppr53 + j*ppr54
  Fppr6 =~ g*ppr61 + h*ppr62 + i*ppr63 + j*ppr64
  
  # Constrained intercepts over time (Strong invariance)
  km11 + km21 + km31 + km41 + km51 + km61 ~ k*1
  km12 + km22 + km32 + km42 + km52 + km62 ~ l*1
  km13 + km23 + km33 + km43 + km53 + km63 ~ m*1
  km14 + km24 + km34 + km44 + km54 + km64 ~ n*1
  km15 + km25 + km35 + km45 + km55 + km65 ~ o*1
  km16 + km26 + km36 + km46 + km56 + km66 ~ p*1
  
  ppr11 + ppr21 + ppr31 + ppr41 + ppr51 + ppr61 ~ q*1
  ppr12 + ppr22 + ppr32 + ppr42 + ppr52 + ppr62 ~ r*1
  ppr13 + ppr23 + ppr33 + ppr43 + ppr53 + ppr63 ~ s*1
  ppr14 + ppr24 + ppr34 + ppr44 + ppr54 + ppr64 ~ t*1
  ppr15 + ppr25 + ppr35 + ppr45 + ppr55 + ppr65 ~ u*1
  ppr16 + ppr26 + ppr36 + ppr46 + ppr56 + ppr66 ~ v*1
  
  # Free latent means from t=2 onward (only do this in combination with constraints on intercepts, above)
  Fkm2 + Fkm3 + Fkm4 + Fkm5 + Fkm6 +
  Fppr1 + Fppr2 + Fppr3 + Fppr4 + Fppr5 + Fppr6 ~ 1
  
  ################
  # BETWEEN PART #
  ################
  
  # Create between factors (RIs)
  RIkm =~ 1*Fkm1 + 1*Fkm2 + 1*Fkm3 + 1*Fkm4 + 1*Fkm5 + 1*Fkm6
  RIppr =~ 1*Fppr1 + 1*Fppr2 + 1*Fppr3 + 1*Fppr4 + 1*Fppr5 + 1*Fppr6
  
  # Set residual variances of all Fkm and Fppr vars to 0
  # I think this specifies all variance of the factors goes to either w/n or b/n
  Fkm1 ~~ 0*Fkm1
  Fkm2 ~~ 0*Fkm2
  Fkm3 ~~ 0*Fkm3
  Fkm4 ~~ 0*Fkm4
  Fkm5 ~~ 0*Fkm5
  Fkm6 ~~ 0*Fkm6
  Fppr1 ~~ 0*Fppr1
  Fppr2 ~~ 0*Fppr2
  Fppr3 ~~ 0*Fppr3
  Fppr4 ~~ 0*Fppr4
  Fppr5 ~~ 0*Fppr5
  Fppr6 ~~ 0*Fppr6

  ###############
  # WITHIN PART #
  ###############

  # Create within-components
  WFkm1 =~ 1*Fkm1
  WFkm2 =~ 1*Fkm2
  WFkm3 =~ 1*Fkm3
  WFkm4 =~ 1*Fkm4
  WFkm5 =~ 1*Fkm5
  WFkm6 =~ 1*Fkm6
  
  WFppr1 =~ 1*Fppr1
  WFppr2 =~ 1*Fppr2
  WFppr3 =~ 1*Fppr3
  WFppr4 =~ 1*Fppr4
  WFppr5 =~ 1*Fppr5
  WFppr6 =~ 1*Fppr6
  
  # Specify lagged effects between within-person centered latent variables
  WFkm2 + WFppr2 ~ WFkm1 + WFppr1
  WFkm3 + WFppr3 ~ WFkm2 + WFppr2
  WFkm4 + WFppr4 ~ WFkm3 + WFppr3
  WFkm5 + WFppr5 ~ WFkm4 + WFppr4
  WFkm6 + WFppr6 ~ WFkm5 + WFppr5
  
  # Estimate correlations within same wave
  WFkm1 ~~ WFppr1 #I am pretty sure they missed this line in the og tutorial
  WFkm2 ~~ WFppr2
  WFkm3 ~~ WFppr3
  WFkm4 ~~ WFppr4
  WFkm5 ~~ WFppr5
  WFkm6 ~~ WFppr6
  
  ##########################
  # ADDITIONAL CONSTRAINTS #
  ##########################
  
  # Set correlations between between-factors (RIs) and within-factors at wave 1 at 0
  RIkm + RIppr ~~ 0*WFkm1 + 0*WFppr1
  
'


base.m3.fit <- cfa(base.m3,
                   data = data,
                   missing = "ML"
                   )
summary(base.m3.fit)
```


### Effects Invariance



## Covariate PA Model

## Covariate RelSat Model